{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyOcQWUqva1QLkKETl+B0d3M",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/fatcrapinmybutt/fredprime-legal-system/blob/main/Copy_of_Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a370e6d5",
    "outputId": "f87ce2d2-9bf6-4c72-a4e8-37513ece5e2a"
   },
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os, sys, json, sqlite3, hashlib, shutil, subprocess, re\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# ---- Auto-detect a real root (prefers D:/ for Windows; falls back for Colab) ----\n",
    "# CANDIDATES = [\"D:/\", \"/content/drive/MyDrive\", \"/content\", str(Path.cwd())]\n",
    "# ROOTS = [Path(p) for p in CANDIDATES if Path(p).exists()]\n",
    "# if not ROOTS:\n",
    "#     print(\"No valid scan root found.\"); sys.exit(1)\n",
    "\n",
    "# For Colab, we will use a default root directory\n",
    "ROOTS = [Path(\"/content/drive/MyDrive/litgation_OS$\")]  # Default root directory\n",
    "\n",
    "ACTION = \"copy\"  # copy|move|link\n",
    "OCR_ENABLED = True\n",
    "BATES_ENABLED = True\n",
    "BATES_PREFIX = \"AJP\"\n",
    "BATES_START = 1\n",
    "\n",
    "RUN_STAMP = datetime.now(timezone.utc).strftime(\"%Y%m%d_%H%M%S\")\n",
    "OUTDIR = Path.cwd() / f\"OUTPUT_{RUN_STAMP}\"\n",
    "\n",
    "REQ = [\n",
    "    \"pandas\",\n",
    "    \"Pillow\",\n",
    "    \"python-docx\",\n",
    "    \"PyPDF2\",\n",
    "    \"pytesseract\",\n",
    "    \"pdfminer.six\",\n",
    "    \"chardet\",\n",
    "]\n",
    "try:\n",
    "    import importlib.util\n",
    "\n",
    "    miss = [p for p in REQ if importlib.util.find_spec(p.split(\".\")[0]) is None]\n",
    "    if miss:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *miss])\n",
    "except Exception as e:\n",
    "    print(\"dep warn:\", e)\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image, ExifTags\n",
    "from docx import Document as DocxDocument\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "import pytesseract, chardet\n",
    "from pdfminer.high_level import extract_text as pdfminer_extract_text\n",
    "\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "\n",
    "def sha256_file(p: Path, buf=1 << 20):\n",
    "    h = hashlib.sha256()\n",
    "    with p.open(\"rb\") as f:\n",
    "        for c in iter(lambda: f.read(buf), b\"\"):\n",
    "            h.update(c)\n",
    "    return h.hexdigest()\n",
    "\n",
    "\n",
    "def detect_encoding(b: bytes):\n",
    "    try:\n",
    "        return (chardet.detect(b) or {}).get(\"encoding\") or \"utf-8\"\n",
    "    except:\n",
    "        return \"utf-8\"\n",
    "\n",
    "\n",
    "def text_preview(s, n=1400):\n",
    "    return re.sub(r\"\\s+\", \" \", s or \"\")[:n]\n",
    "\n",
    "\n",
    "def exif_dict(img):\n",
    "    try:\n",
    "        return {\n",
    "            ExifTags.TAGS.get(k, str(k)): (v.decode() if isinstance(v, bytes) else v)\n",
    "            for k, v in (img.getexif() or {}).items()\n",
    "        }\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "\n",
    "class Org:\n",
    "    def __init__(self):\n",
    "        self.roots = ROOTS\n",
    "        self.out = ensure_dir(OUTDIR)\n",
    "        self.org = ensure_dir(self.out / \"ORGANIZED\")\n",
    "        self.ocr = ensure_dir(self.out / \"OCR\")\n",
    "        self.bates_dir = ensure_dir(self.out / \"BATES\")\n",
    "        self.db = sqlite3.connect(self.out / \"evidence.db\")\n",
    "        self.db.execute(\n",
    "            \"CREATE TABLE IF NOT EXISTS files(sha TEXT PRIMARY KEY, path TEXT, organized TEXT, bates TEXT, meta TEXT)\"\n",
    "        )\n",
    "        self.bates = BATES_START\n",
    "\n",
    "    def meta(self, p: Path):\n",
    "        e = p.suffix.lower()\n",
    "        try:\n",
    "            if e == \".pdf\":\n",
    "                m = {\"pages\": len(PdfReader(str(p)).pages)}\n",
    "                try:\n",
    "                    m[\"text\"] = text_preview(pdfminer_extract_text(str(p)))\n",
    "                except:\n",
    "                    pass\n",
    "                return m\n",
    "            if e == \".docx\":\n",
    "                d = DocxDocument(str(p))\n",
    "                return {\"text\": text_preview(\"\\n\".join(x.text for x in d.paragraphs))}\n",
    "            if e in (\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\", \".bmp\", \".gif\", \".webp\"):\n",
    "                with Image.open(str(p)) as im:\n",
    "                    m = {\n",
    "                        \"format\": im.format,\n",
    "                        \"mode\": im.mode,\n",
    "                        \"size\": im.size,\n",
    "                        \"exif\": exif_dict(im),\n",
    "                    }\n",
    "                    if OCR_ENABLED:\n",
    "                        try:\n",
    "                            txt = pytesseract.image_to_string(im)\n",
    "                            (self.ocr / (p.stem + \".txt\")).write_text(\n",
    "                                txt, encoding=\"utf-8\", errors=\"ignore\"\n",
    "                            )\n",
    "                            m[\"ocr\"] = text_preview(txt)\n",
    "                        except Exception as ex:\n",
    "                            m[\"ocr_err\"] = str(ex)\n",
    "                    return m\n",
    "            if e in (\n",
    "                \".txt\",\n",
    "                \".log\",\n",
    "                \".md\",\n",
    "                \".json\",\n",
    "                \".jsonl\",\n",
    "                \".csv\",\n",
    "                \".ini\",\n",
    "                \".conf\",\n",
    "                \".yml\",\n",
    "                \".yaml\",\n",
    "            ):\n",
    "                b = p.read_bytes()[:400_000]\n",
    "                return {\"text\": text_preview(b.decode(detect_encoding(b), \"ignore\"))}\n",
    "        except Exception as ex:\n",
    "            return {\"err\": str(ex)}\n",
    "        return {}\n",
    "\n",
    "    def place(self, src: Path, sha: str):\n",
    "        dst = self.org / sha[:2] / (sha + src.suffix.lower())\n",
    "        ensure_dir(dst.parent)\n",
    "        if not dst.exists():\n",
    "            try:\n",
    "                if ACTION == \"copy\":\n",
    "                    shutil.copy2(src, dst)\n",
    "                elif ACTION == \"move\":\n",
    "                    shutil.move(src, dst)\n",
    "                elif ACTION == \"link\":\n",
    "                    os.link(src, dst)\n",
    "                else:\n",
    "                    shutil.copy2(src, dst)\n",
    "            except Exception:\n",
    "                shutil.copy2(src, dst)\n",
    "        return dst\n",
    "\n",
    "    def run(self):\n",
    "        recs = []\n",
    "        for root in self.roots:\n",
    "            for dp, _, files in os.walk(root):\n",
    "                for name in files:\n",
    "                    p = Path(dp) / name\n",
    "                    if self.out in p.parents:\n",
    "                        continue\n",
    "                    try:\n",
    "                        sha = sha256_file(p)\n",
    "                        meta = self.meta(p)\n",
    "                        dst = self.place(p, sha)\n",
    "                        bid = \"\"\n",
    "                        if BATES_ENABLED and p.suffix.lower() == \".pdf\":\n",
    "                            bid = f\"AJP-{self.bates:07d}\"\n",
    "                            self.bates += 1\n",
    "                            shutil.copy2(dst, self.bates_dir / f\"{sha}_{bid}.pdf\")\n",
    "                        self.db.execute(\n",
    "                            \"INSERT OR REPLACE INTO files VALUES(?,?,?,?,?)\",\n",
    "                            (\n",
    "                                sha,\n",
    "                                str(p),\n",
    "                                str(dst),\n",
    "                                bid,\n",
    "                                json.dumps(meta, ensure_ascii=False),\n",
    "                            ),\n",
    "                        )\n",
    "                        recs.append(\n",
    "                            {\n",
    "                                \"path\": str(p),\n",
    "                                \"organized\": str(dst),\n",
    "                                \"sha\": sha,\n",
    "                                \"bates\": bid,\n",
    "                                **meta,\n",
    "                            }\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        print(\"fail:\", p, e)\n",
    "        self.db.commit()\n",
    "        self.db.close()\n",
    "        pd.DataFrame(recs).to_csv(self.out / \"evidence.csv\", index=False)\n",
    "        (self.out / \"MANIFEST.json\").write_text(\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"roots\": [str(r) for r in self.roots],\n",
    "                    \"scanned_root\": str(self.roots[0]),\n",
    "                    \"outdir\": str(self.out),\n",
    "                    \"files\": len(recs),\n",
    "                    \"utc_finished\": datetime.now(timezone.utc).isoformat(),\n",
    "                },\n",
    "                indent=2,\n",
    "            )\n",
    "        )\n",
    "        print(\"[scan root]\", self.roots[0])\n",
    "        print(\"[done]\", self.out)\n",
    "\n",
    "\n",
    "Org().run()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yDv2oyQBN536",
    "outputId": "cfa0abf7-17ab-4a5e-d633-87c9d31a3a00"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[scan root] /content/drive/MyDrive/litgation_OS$\n",
      "[done] /content/OUTPUT_20250908_170554\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "61124d6b",
    "outputId": "2076343c-a168-43ee-b359-d81bb4806c45"
   },
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Get the latest output directory name from the previous run's stdout\n",
    "output_dirs = [d for d in os.listdir(\"/content\") if d.startswith(\"OUTPUT_\")]\n",
    "latest_output_dir = sorted(output_dirs)[-1] if output_dirs else None\n",
    "\n",
    "if latest_output_dir:\n",
    "    csv_path = os.path.join(\"/content\", latest_output_dir, \"evidence.csv\")\n",
    "    if os.path.exists(csv_path):\n",
    "        df_evidence = pd.read_csv(csv_path)\n",
    "        display(df_evidence)\n",
    "    else:\n",
    "        print(f\"Error: {csv_path} not found.\")\n",
    "else:\n",
    "    print(\"Error: No OUTPUT directory found.\")"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "error",
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-626450329.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mcsv_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatest_output_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'evidence.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mdf_evidence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_evidence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1897\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1898\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1899\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1900\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;31m# Fail here loudly instead of in cython after reading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pyarrow\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BZN5JHeeJ5ld",
    "outputId": "6716dc81-e577-4eaa-bc71-df5ebf76b421"
   },
   "source": [
    "#!/usr/bin/env python3\n",
    "# EVIDENCE_ORGANIZER.py\n",
    "# Purpose: Forensically ingest, organize, and process evidence files.\n",
    "# Outputs:\n",
    "#   OUTPUT/\n",
    "#     evidence.db                  (SQLite)\n",
    "#     evidence.csv                 (flat table)\n",
    "#     evidence.jsonl               (full JSON lines)\n",
    "#     MANIFEST.json                (summary + run config)\n",
    "#     LEDGER.jsonl                 (chain-of-custody style events)\n",
    "#     ORGANIZED/                   (copy|move|link by type/date/hash)\n",
    "#     UNPACKED/                    (safe archive extraction)\n",
    "#     THUMBNAILS/                  (image thumbs)\n",
    "#     OCR/                         (OCR text for images and image-only PDFs)\n",
    "#     BATES/                       (Bates-stamped PDF copies if enabled)\n",
    "#\n",
    "# Dependencies auto-install (if allowed): pip, pandas, pillow, python-docx, PyPDF2, pikepdf, pytesseract, pdfminer.six, chardet, tqdm\n",
    "# Optional external tools: tesseract-ocr, ffprobe (from ffmpeg), exiftool (fallback)\n",
    "#\n",
    "# Usage:\n",
    "#   python EVIDENCE_ORGANIZER.py --root \"F:\\Evidence\" --action copy --unpack --bates \"AJP\" --bates-start 1\n",
    "#   python EVIDENCE_ORGANIZER.py --root \"/path/in\" \"/other/path\" --threads 8 --no-ocr --no-bates\n",
    "#\n",
    "# Notes:\n",
    "# - Originals are never modified. Organized tree is separate.\n",
    "# - Bates stamping overlays page footer if ReportLab present; else filename-only Bates copies with mapping are created.\n",
    "# - OCR requires Tesseract installed and on PATH. Without it, OCR is skipped gracefully.\n",
    "# - Archive extraction is safe-mode: blocks absolute paths and .. traversal.\n",
    "# - Duplicate detection by SHA256. Duplicates listed and can be linked/copied to ORGANIZED/DEDUP but not deleted.\n",
    "\n",
    "import os, sys, json, sqlite3, hashlib, zipfile, tarfile, io, shutil, subprocess, argparse, re, time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# ---------- Dependency management ----------\n",
    "REQUIRED_PY = [\n",
    "    (\"pandas\", \"pandas\"),\n",
    "    (\"PIL\", \"Pillow\"),\n",
    "    (\"docx\", \"python-docx\"),\n",
    "    (\"PyPDF2\", \"PyPDF2\"),\n",
    "    (\"pikepdf\", \"pikepdf\"),\n",
    "    (\"pytesseract\", \"pytesseract\"),\n",
    "    (\"pdfminer\", \"pdfminer.six\"),\n",
    "    (\"chardet\", \"chardet\"),\n",
    "    (\"tqdm\", \"tqdm\"),\n",
    "]\n",
    "\n",
    "\n",
    "def ensure_deps():\n",
    "    try:\n",
    "        import importlib\n",
    "\n",
    "        missing = []\n",
    "        for mod, pkg in REQUIRED_PY:\n",
    "            try:\n",
    "                importlib.import_module(mod)\n",
    "            except Exception:\n",
    "                missing.append(pkg)\n",
    "        if missing:\n",
    "            # Try installing\n",
    "            print(f\"[deps] Installing: {missing}\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *missing])\n",
    "    except Exception as e:\n",
    "        print(f\"[deps] Warning: failed auto-install: {e}\")\n",
    "\n",
    "\n",
    "ensure_deps()\n",
    "\n",
    "# Now import installed modules\n",
    "import pandas as pd\n",
    "from PIL import Image, ExifTags\n",
    "from docx import Document as DocxDocument\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "import pytesseract\n",
    "from pdfminer.high_level import extract_text as pdfminer_extract_text\n",
    "import chardet\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "APP_VERSION = \"1.7.0\"\n",
    "NOW_ISO = datetime.utcnow().replace(microsecond=0).isoformat() + \"Z\"\n",
    "SAFE_ARCHIVE_EXT = {\n",
    "    \".zip\",\n",
    "    \".tar\",\n",
    "    \".tgz\",\n",
    "    \".tar.gz\",\n",
    "    \".tar.bz2\",\n",
    "    \".tar.xz\",\n",
    "    \".7z\",\n",
    "}  # 7z read via external if present\n",
    "DOC_EXT = {\".docx\", \".doc\", \".rtf\", \".odt\", \".pages\"}\n",
    "PDF_EXT = {\".pdf\"}\n",
    "IMG_EXT = {\".jpg\", \".jpeg\", \".png\", \".tiff\", \".tif\", \".bmp\", \".gif\", \".webp\", \".heic\"}\n",
    "TXT_EXT = {\n",
    "    \".txt\",\n",
    "    \".md\",\n",
    "    \".csv\",\n",
    "    \".log\",\n",
    "    \".json\",\n",
    "    \".jsonl\",\n",
    "    \".yml\",\n",
    "    \".yaml\",\n",
    "    \".ini\",\n",
    "    \".conf\",\n",
    "}\n",
    "AV_EXT = {\n",
    "    \".mp3\",\n",
    "    \".wav\",\n",
    "    \".m4a\",\n",
    "    \".aac\",\n",
    "    \".flac\",\n",
    "    \".ogg\",\n",
    "    \".mp4\",\n",
    "    \".mkv\",\n",
    "    \".mov\",\n",
    "    \".avi\",\n",
    "    \".wmv\",\n",
    "    \".webm\",\n",
    "}\n",
    "\n",
    "\n",
    "def sha256_file(p: Path, buf_size=1024 * 1024) -> str:\n",
    "    h = hashlib.sha256()\n",
    "    with p.open(\"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(buf_size)\n",
    "            if not b:\n",
    "                break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "\n",
    "def bytes_safe_read(p: Path, limit=5_000_000):\n",
    "    with p.open(\"rb\") as f:\n",
    "        return f.read(limit)\n",
    "\n",
    "\n",
    "def detect_encoding(data: bytes):\n",
    "    try:\n",
    "        if not data:\n",
    "            return \"utf-8\"\n",
    "        d = chardet.detect(data)\n",
    "        return d.get(\"encoding\") or \"utf-8\"\n",
    "    except:\n",
    "        return \"utf-8\"\n",
    "\n",
    "\n",
    "def run_ffprobe(path: Path):\n",
    "    try:\n",
    "        cmd = [\n",
    "            \"ffprobe\",\n",
    "            \"-v\",\n",
    "            \"error\",\n",
    "            \"-print_format\",\n",
    "            \"json\",\n",
    "            \"-show_format\",\n",
    "            \"-show_streams\",\n",
    "            str(path),\n",
    "        ]\n",
    "        out = subprocess.check_output(cmd, stderr=subprocess.STDOUT, timeout=30)\n",
    "        return json.loads(out.decode(\"utf-8\", \"ignore\"))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def have_reportlab():\n",
    "    try:\n",
    "        import reportlab\n",
    "\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def text_preview(s: str, n=1200):\n",
    "    s = re.sub(r\"\\s+\", \" \", s or \"\")\n",
    "    return s[:n]\n",
    "\n",
    "\n",
    "def exif_dict(img: Image.Image):\n",
    "    try:\n",
    "        raw = img.getexif()\n",
    "        if not raw:\n",
    "            return {}\n",
    "        label = {}\n",
    "        for k, v in raw.items():\n",
    "            nm = ExifTags.TAGS.get(k, str(k))\n",
    "            # Convert bytes metadata safely\n",
    "            if isinstance(v, bytes):\n",
    "                try:\n",
    "                    v = v.decode(\"utf-8\", \"ignore\")\n",
    "                except:\n",
    "                    v = str(v)\n",
    "            label[nm] = v\n",
    "        return label\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "\n",
    "def is_image_only_pdf(pdf_path: Path):\n",
    "    try:\n",
    "        reader = PdfReader(str(pdf_path))\n",
    "        # Heuristic: pages with no text via pdfminer extraction\n",
    "        # Short-circuit: if pdfminer finds any text, treat as not image-only\n",
    "        txt = pdfminer_extract_text(str(pdf_path))\n",
    "        if txt and txt.strip():\n",
    "            return False\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def safe_extract_zip(zp: Path, outdir: Path):\n",
    "    with zipfile.ZipFile(zp, \"r\") as z:\n",
    "        for m in z.infolist():\n",
    "            # guard\n",
    "            name = m.filename\n",
    "            if name.startswith(\"/\") or \"..\" in Path(name).parts:\n",
    "                continue\n",
    "            dest = outdir / name\n",
    "            dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "            with z.open(m, \"r\") as src, dest.open(\"wb\") as dst:\n",
    "                shutil.copyfileobj(src, dst)\n",
    "\n",
    "\n",
    "def safe_extract_tar(tp: Path, outdir: Path):\n",
    "    mode = \"r\"\n",
    "    if str(tp).endswith(\".tar.gz\") or str(tp).endswith(\".tgz\"):\n",
    "        mode = \"r:gz\"\n",
    "    elif str(tp).endswith(\".tar.bz2\"):\n",
    "        mode = \"r:bz2\"\n",
    "    elif str(tp).endswith(\".tar.xz\"):\n",
    "        mode = \"r:xz\"\n",
    "    with tarfile.open(tp, mode) as t:\n",
    "        for m in t.getmembers():\n",
    "            name = m.name\n",
    "            if name.startswith(\"/\") or \"..\" in Path(name).parts:\n",
    "                continue\n",
    "            dest = outdir / name\n",
    "            if m.isdir():\n",
    "                dest.mkdir(parents=True, exist_ok=True)\n",
    "            else:\n",
    "                dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "                with t.extractfile(m) as src, dest.open(\"wb\") as dst:\n",
    "                    if src:\n",
    "                        shutil.copyfileobj(src, dst)\n",
    "\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "\n",
    "def bates_id(prefix: str, n: int) -> str:\n",
    "    return f\"{prefix}-{n:07d}\"\n",
    "\n",
    "\n",
    "# ---------- Core processor ----------\n",
    "class EvidenceOrganizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        roots,\n",
    "        outdir,\n",
    "        threads=6,\n",
    "        action=\"copy\",\n",
    "        do_ocr=True,\n",
    "        do_unpack=False,\n",
    "        do_bates=True,\n",
    "        bates_prefix=\"BATES\",\n",
    "        bates_start=1,\n",
    "    ):\n",
    "        self.roots = [Path(r).resolve() for r in roots]\n",
    "        self.outdir = Path(outdir).resolve()\n",
    "        self.db_path = self.outdir / \"evidence.db\"\n",
    "        self.csv_path = self.outdir / \"evidence.csv\"\n",
    "        self.jsonl_path = self.outdir / \"evidence.jsonl\"\n",
    "        self.manifest_path = self.outdir / \"MANIFEST.json\"\n",
    "        self.ledger_path = self.outdir / \"LEDGER.jsonl\"\n",
    "        self.org_dir = ensure_dir(self.outdir / \"ORGANIZED\")\n",
    "        self.unpack_dir = ensure_dir(self.outdir / \"UNPACKED\")\n",
    "        self.thumb_dir = ensure_dir(self.outdir / \"THUMBNAILS\")\n",
    "        self.ocr_dir = ensure_dir(self.outdir / \"OCR\")\n",
    "        self.bates_dir = ensure_dir(self.outdir / \"BATES\")\n",
    "        self.dup_dir = ensure_dir(self.org_dir / \"DEDUP\")\n",
    "        self.threads = threads\n",
    "        self.action = action  # copy|move|link\n",
    "        self.do_ocr = do_ocr\n",
    "        self.do_unpack = do_unpack\n",
    "        self.do_bates = do_bates\n",
    "        self.bates_prefix = bates_prefix\n",
    "        self.bates_counter = bates_start\n",
    "        self.db = None\n",
    "        self._init_db()\n",
    "\n",
    "    # ----- DB -----\n",
    "    def _init_db(self):\n",
    "        ensure_dir(self.outdir)\n",
    "        self.db = sqlite3.connect(self.db_path)\n",
    "        self.db.execute(\n",
    "            \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS files(\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            path TEXT NOT NULL,\n",
    "            relpath TEXT NOT NULL,\n",
    "            root TEXT NOT NULL,\n",
    "            size INTEGER,\n",
    "            mtime REAL,\n",
    "            ctime REAL,\n",
    "            ext TEXT,\n",
    "            mime TEXT,\n",
    "            sha256 TEXT UNIQUE,\n",
    "            type TEXT,      -- class: pdf/doc/img/av/txt/other\n",
    "            meta_json TEXT, -- extracted metadata\n",
    "            ocr_path TEXT,  -- OCR output text file if any\n",
    "            organized_path TEXT, -- copy/move/link target\n",
    "            bates_id TEXT   -- if stamped\n",
    "        );\"\"\"\n",
    "        )\n",
    "        self.db.execute(\n",
    "            \"CREATE UNIQUE INDEX IF NOT EXISTS idx_sha256 ON files(sha256);\"\n",
    "        )\n",
    "        self.db.execute(\"CREATE INDEX IF NOT EXISTS idx_type ON files(type);\")\n",
    "        self.db.commit()\n",
    "\n",
    "    def _insert_or_update(self, rec: dict):\n",
    "        cols = [\n",
    "            \"path\",\n",
    "            \"relpath\",\n",
    "            \"root\",\n",
    "            \"size\",\n",
    "            \"mtime\",\n",
    "            \"ctime\",\n",
    "            \"ext\",\n",
    "            \"mime\",\n",
    "            \"sha256\",\n",
    "            \"type\",\n",
    "            \"meta_json\",\n",
    "            \"ocr_path\",\n",
    "            \"organized_path\",\n",
    "            \"bates_id\",\n",
    "        ]\n",
    "        vals = [rec.get(k) for k in cols]\n",
    "        try:\n",
    "            self.db.execute(\n",
    "                f\"\"\"\n",
    "                INSERT INTO files({\",\".join(cols)}) VALUES ({\",\".join([\"?\"]*len(cols))})\n",
    "                ON CONFLICT(sha256) DO UPDATE SET\n",
    "                    path=excluded.path,\n",
    "                    relpath=excluded.relpath,\n",
    "                    root=excluded.root,\n",
    "                    size=excluded.size,\n",
    "                    mtime=excluded.mtime,\n",
    "                    ctime=excluded.ctime,\n",
    "                    ext=excluded.ext,\n",
    "                    mime=excluded.mime,\n",
    "                    type=excluded.type,\n",
    "                    meta_json=excluded.meta_json,\n",
    "                    ocr_path=excluded.ocr_path,\n",
    "                    organized_path=excluded.organized_path,\n",
    "                    bates_id=excluded.bates_id\n",
    "            \"\"\",\n",
    "                vals,\n",
    "            )\n",
    "            self.db.commit()\n",
    "        except sqlite3.Error as e:\n",
    "            print(f\"[db] {e} for {rec.get('path')}\")\n",
    "\n",
    "    # ----- Chain-of-custody ledger -----\n",
    "    def log_event(self, ev: dict):\n",
    "        ev = dict(ev)\n",
    "        ev[\"ts\"] = datetime.utcnow().isoformat() + \"Z\"\n",
    "        with self.ledger_path.open(\"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(ev, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    # ----- File classification -----\n",
    "    def classify(self, p: Path):\n",
    "        ext = p.suffix.lower()\n",
    "        if ext in PDF_EXT:\n",
    "            return \"pdf\"\n",
    "        if ext in DOC_EXT:\n",
    "            return \"doc\"\n",
    "        if ext in IMG_EXT:\n",
    "            return \"img\"\n",
    "        if ext in TXT_EXT:\n",
    "            return \"txt\"\n",
    "        if ext in AV_EXT:\n",
    "            return \"av\"\n",
    "        return \"other\"\n",
    "\n",
    "    # ----- Metadata extractors -----\n",
    "    def meta_pdf(self, p: Path):\n",
    "        d = {}\n",
    "        try:\n",
    "            reader = PdfReader(str(p))\n",
    "            info = reader.metadata or {}\n",
    "            d[\"pages\"] = len(reader.pages)\n",
    "            d[\"pdf_info\"] = {k: str(v) for k, v in info.items()}\n",
    "            # quick text preview\n",
    "            try:\n",
    "                txt = pdfminer_extract_text(str(p))\n",
    "                d[\"text_preview\"] = text_preview(txt)\n",
    "            except Exception:\n",
    "                d[\"text_preview\"] = \"\"\n",
    "        except Exception as e:\n",
    "            d[\"error\"] = f\"{e}\"\n",
    "        return d\n",
    "\n",
    "    def meta_doc(self, p: Path):\n",
    "        d = {}\n",
    "        try:\n",
    "            if p.suffix.lower() == \".docx\":\n",
    "                doc = DocxDocument(str(p))\n",
    "                props = doc.core_properties\n",
    "                d[\"docx_props\"] = {\n",
    "                    k: getattr(props, k)\n",
    "                    for k in dir(props)\n",
    "                    if not k.startswith(\"_\") and not callable(getattr(props, k))\n",
    "                }\n",
    "                # text preview\n",
    "                chunks = []\n",
    "                for para in doc.paragraphs[:200]:\n",
    "                    if para.text:\n",
    "                        chunks.append(para.text)\n",
    "                d[\"text_preview\"] = text_preview(\"\\n\".join(chunks))\n",
    "            else:\n",
    "                # best effort using bytes read + encoding detect\n",
    "                data = bytes_safe_read(p, limit=1_500_000)\n",
    "                enc = detect_encoding(data)\n",
    "                d[\"raw_preview\"] = text_preview(data.decode(enc, \"ignore\"))\n",
    "        except Exception as e:\n",
    "            d[\"error\"] = f\"{e}\"\n",
    "        return d\n",
    "\n",
    "    def meta_img(self, p: Path):\n",
    "        d = {}\n",
    "        try:\n",
    "            with Image.open(str(p)) as im:\n",
    "                d[\"format\"] = im.format\n",
    "                d[\"mode\"] = im.mode\n",
    "                d[\"size\"] = im.size\n",
    "                d[\"exif\"] = exif_dict(im)\n",
    "                # thumbnail\n",
    "                th = self.thumb_dir / (p.stem + \"_thumb.jpg\")\n",
    "                im2 = im.copy()\n",
    "                im2.thumbnail((800, 800))\n",
    "                im2.save(th, \"JPEG\", quality=85)\n",
    "                d[\"thumbnail\"] = str(th)\n",
    "                # OCR\n",
    "                if self.do_ocr:\n",
    "                    try:\n",
    "                        txt = pytesseract.image_to_string(im2)\n",
    "                        ocrp = self.ocr_dir / (p.stem + \".txt\")\n",
    "                        ocrp.write_text(txt, encoding=\"utf-8\", errors=\"ignore\")\n",
    "                        d[\"ocr_path\"] = str(ocrp)\n",
    "                        d[\"text_preview\"] = text_preview(txt)\n",
    "                    except Exception as e:\n",
    "                        d[\"ocr_error\"] = str(e)\n",
    "        except Exception as e:\n",
    "            d[\"error\"] = f\"{e}\"\n",
    "        return d\n",
    "\n",
    "    def meta_txt(self, p: Path):\n",
    "        d = {}\n",
    "        try:\n",
    "            data = bytes_safe_read(p, limit=5_000_000)\n",
    "            enc = detect_encoding(data)\n",
    "            s = data.decode(enc, \"ignore\")\n",
    "            d[\"encoding\"] = enc\n",
    "            d[\"text_preview\"] = text_preview(s, n=4000)\n",
    "        except Exception as e:\n",
    "            d[\"error\"] = f\"{e}\"\n",
    "        return d\n",
    "\n",
    "    def meta_av(self, p: Path):\n",
    "        d = {}\n",
    "        probe = run_ffprobe(p)\n",
    "        if probe:\n",
    "            d[\"ffprobe\"] = probe\n",
    "        else:\n",
    "            d[\"note\"] = \"ffprobe not available\"\n",
    "        return d\n",
    "\n",
    "    # ----- Organize copy/move/link -----\n",
    "    def organize_target(self, rec: dict):\n",
    "        ftype = rec[\"type\"] or \"other\"\n",
    "        sha = rec[\"sha256\"]\n",
    "        ext = rec[\"ext\"] or \"\"\n",
    "        dt_dir = datetime.utcfromtimestamp(rec[\"mtime\"] or time.time()).strftime(\n",
    "            \"%Y/%m/%d\"\n",
    "        )\n",
    "        base = f\"{sha[:12]}{ext}\"\n",
    "        target = self.org_dir / ftype / dt_dir / base\n",
    "        ensure_dir(target.parent)\n",
    "        return target\n",
    "\n",
    "    def place_file(self, src: Path, dst: Path, mode: str):\n",
    "        if dst.exists():\n",
    "            return\n",
    "        if mode == \"copy\":\n",
    "            shutil.copy2(src, dst)\n",
    "        elif mode == \"move\":\n",
    "            shutil.move(src, dst)\n",
    "        elif mode == \"link\":\n",
    "            try:\n",
    "                os.link(src, dst)\n",
    "            except Exception:\n",
    "                shutil.copy2(src, dst)\n",
    "        else:\n",
    "            shutil.copy2(src, dst)\n",
    "        self.log_event({\"op\": \"place\", \"mode\": mode, \"src\": str(src), \"dst\": str(dst)})\n",
    "\n",
    "    # ----- Bates stamping -----\n",
    "    def bates_pdf_overlay(self, pdf_in: Path, pdf_out: Path, bates: str):\n",
    "        if not have_reportlab():\n",
    "            # Fallback: copy only; rename includes bates id\n",
    "            shutil.copy2(pdf_in, pdf_out)\n",
    "            return \"filename_only\"\n",
    "        # Build 1-page footer overlay and merge on each page\n",
    "        from reportlab.pdfgen import canvas\n",
    "        from reportlab.lib.pagesizes import letter\n",
    "        from reportlab.lib.units import inch\n",
    "\n",
    "        tmp_overlay = pdf_out.parent / (pdf_out.stem + \"_overlay.pdf\")\n",
    "        # Create overlay\n",
    "        c = canvas.Canvas(str(tmp_overlay), pagesize=letter)\n",
    "        w, h = letter\n",
    "        c.setFont(\"Helvetica\", 10)\n",
    "        c.drawString(0.7 * inch, 0.5 * inch, f\"{bates}\")\n",
    "        c.save()\n",
    "        # Merge\n",
    "        reader = PdfReader(str(pdf_in))\n",
    "        overlay = PdfReader(str(tmp_overlay))\n",
    "        over_pg = overlay.pages[0]\n",
    "        writer = PdfWriter()\n",
    "        for pg in reader.pages:\n",
    "            pg.merge_page(over_pg)\n",
    "            writer.add_page(pg)\n",
    "        with pdf_out.open(\"wb\") as f:\n",
    "            writer.write(f)\n",
    "        tmp_overlay.unlink(missing_ok=True)\n",
    "        return \"overlay\"\n",
    "\n",
    "    # ----- Archive unpack -----\n",
    "    def maybe_unpack(self, p: Path):\n",
    "        if not self.do_unpack:\n",
    "            return\n",
    "        ext = p.suffix.lower()\n",
    "        try:\n",
    "            if ext == \".zip\":\n",
    "                out = self.unpack_dir / (p.stem + \"_unzipped\")\n",
    "                ensure_dir(out)\n",
    "                safe_extract_zip(p, out)\n",
    "                self.log_event({\"op\": \"unpack_zip\", \"src\": str(p), \"dst\": str(out)})\n",
    "            elif ext in {\n",
    "                \".tar\",\n",
    "                \".tgz\",\n",
    "                \".gz\",\n",
    "                \".bz2\",\n",
    "                \".xz\",\n",
    "                \".tar.gz\",\n",
    "                \".tar.bz2\",\n",
    "                \".tar.xz\",\n",
    "            }:\n",
    "                out = self.unpack_dir / (p.stem + \"_untarred\")\n",
    "                ensure_dir(out)\n",
    "                safe_extract_tar(p, out)\n",
    "                self.log_event({\"op\": \"unpack_tar\", \"src\": str(p), \"dst\": str(out)})\n",
    "            elif ext == \".7z\":\n",
    "                # requires 7z in PATH\n",
    "                out = self.unpack_dir / (p.stem + \"_7z\")\n",
    "                ensure_dir(out)\n",
    "                try:\n",
    "                    subprocess.check_call(\n",
    "                        [\"7z\", \"x\", \"-y\", str(p), f\"-o{out}\"], timeout=120\n",
    "                    )\n",
    "                    self.log_event(\n",
    "                        {\"op\": \"unpack_7z_fail\", \"src\": str(p), \"err\": str(e)}\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    self.log_event(\n",
    "                        {\"op\": \"unpack_7z_fail\", \"src\": str(p), \"err\": str(e)}\n",
    "                    )\n",
    "        except Exception as e:\n",
    "            self.log_event({\"op\": \"unpack_fail\", \"src\": str(p), \"err\": str(e)})\n",
    "\n",
    "    # ----- Per-file pipeline -----\n",
    "    def process_one(self, p: Path, root: Path):\n",
    "        try:\n",
    "            stat = p.stat()\n",
    "            ext = p.suffix.lower()\n",
    "            ftype = self.classify(p)\n",
    "            sha = sha256_file(p)\n",
    "            mime = None  # minimized; Python-magic optional\n",
    "            meta = {}\n",
    "            ocr_path = None\n",
    "\n",
    "            if ftype == \"pdf\":\n",
    "                meta = self.meta_pdf(p)\n",
    "                # OCR image-only PDFs\n",
    "                if self.do_ocr and is_image_only_pdf(p):\n",
    "                    try:\n",
    "                        # Extract per-page images then OCR via Tesseract's PDF mode if available\n",
    "                        # Simple fallback: pdfminer text already empty; we run tesseract directly to text\n",
    "                        ocr_txt = \"\"\n",
    "                        try:\n",
    "                            out_txt = self.ocr_dir / (p.stem + \"_pdf_ocr.txt\")\n",
    "                            subprocess.check_call(\n",
    "                                [\n",
    "                                    \"tesseract\",\n",
    "                                    str(p),\n",
    "                                    str(out_txt.with_suffix(\"\")),\n",
    "                                    \"pdf\",\n",
    "                                    \"txt\",\n",
    "                                ],\n",
    "                                timeout=240,\n",
    "                            )\n",
    "                            # Tesseract writes .txt automatically\n",
    "                            if out_txt.exists():\n",
    "                                ocr_txt = out_txt.read_text(\n",
    "                                    encoding=\"utf-8\", errors=\"ignore\"\n",
    "                                )\n",
    "                                ocr_path = str(out_txt)\n",
    "                        except Exception:\n",
    "                            # Final fallback: no OCR\n",
    "                            pass\n",
    "                        if ocr_txt:\n",
    "                            meta[\"ocr_text_preview\"] = text_preview(ocr_txt)\n",
    "                    except Exception as e:\n",
    "                        meta[\"ocr_error\"] = str(e)\n",
    "\n",
    "            elif ftype == \"doc\":\n",
    "                meta = self.meta_doc(p)\n",
    "            elif ftype == \"img\":\n",
    "                meta = self.meta_img(p)\n",
    "                ocr_path = meta.get(\"ocr_path\")\n",
    "            elif ftype == \"txt\":\n",
    "                meta = self.meta_txt(p)\n",
    "            elif ftype == \"av\":\n",
    "                meta = self.meta_av(p)\n",
    "            else:\n",
    "                # try small preview\n",
    "                try:\n",
    "                    data = bytes_safe_read(p, limit=512_000)\n",
    "                    enc = detect_encoding(data)\n",
    "                    s = data.decode(enc, \"ignore\")\n",
    "                    meta = {\"raw_preview\": text_preview(s)}\n",
    "                except Exception:\n",
    "                    meta = {}\n",
    "\n",
    "            # organize\n",
    "            rec = {\n",
    "                \"path\": str(p),\n",
    "                \"relpath\": str(p.relative_to(root)),\n",
    "                \"root\": str(root),\n",
    "                \"size\": stat.st_size,\n",
    "                \"mtime\": stat.st_mtime,\n",
    "                \"ctime\": stat.st_ctime,\n",
    "                \"ext\": ext,\n",
    "                \"mime\": mime or \"\",\n",
    "                \"sha256\": sha,\n",
    "                \"type\": ftype,\n",
    "                \"meta_json\": json.dumps(meta, ensure_ascii=False),\n",
    "                \"ocr_path\": ocr_path or \"\",\n",
    "                \"organized_path\": \"\",\n",
    "                \"bates_id\": \"\",\n",
    "            }\n",
    "\n",
    "            # place organized copy\n",
    "            dst = self.organize_target(rec)\n",
    "            self.place_file(p, dst, self.action)\n",
    "            rec[\"organized_path\"] = str(dst)\n",
    "\n",
    "            # maybe unpack archives\n",
    "            if ext in SAFE_ARCHIVE_EXT:\n",
    "                self.maybe_unpack(p)\n",
    "\n",
    "            # maybe bates-stamp PDFs\n",
    "            if self.do_bates and ftype == \"pdf\":\n",
    "                bid = bates_id(self.bates_prefix, self.bates_counter)\n",
    "                self.bates_counter += 1\n",
    "                outpdf = self.bates_dir / f\"{dst.stem}_{bid}.pdf\"\n",
    "                mode = self.bates_pdf_overlay(dst, outpdf, bid)\n",
    "                rec[\"bates_id\"] = bid\n",
    "                self.log_event(\n",
    "                    {\n",
    "                        \"op\": \"bates\",\n",
    "                        \"src\": str(dst),\n",
    "                        \"dst\": str(outpdf),\n",
    "                        \"bates_id\": bid,\n",
    "                        \"mode\": mode,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            # write db\n",
    "            self._insert_or_update(rec)\n",
    "            return rec\n",
    "\n",
    "        except Exception as e:\n",
    "            self.log_event({\"op\": \"process_fail\", \"path\": str(p), \"err\": str(e)})\n",
    "            return None\n",
    "\n",
    "    # ----- Walk -----\n",
    "    def iter_files(self):\n",
    "        # Limit the number of root directories processed\n",
    "        root_limit = 20  # Set your desired limit here\n",
    "        processed_roots_count = 0\n",
    "        for root in self.roots:\n",
    "            if processed_roots_count >= root_limit:\n",
    "                break\n",
    "            if root.is_dir():\n",
    "                processed_roots_count += 1\n",
    "                for dirpath, _, filenames in os.walk(root):\n",
    "                    for name in filenames:\n",
    "                        p = Path(dirpath) / name\n",
    "                        # skip our own OUTPUT\n",
    "                        if self.outdir in p.parents:\n",
    "                            continue\n",
    "                        yield p, root\n",
    "\n",
    "    # ----- Run -----\n",
    "    def run(self):\n",
    "        # manifest start\n",
    "        manifest = {\n",
    "            \"version\": APP_VERSION,\n",
    "            \"started\": NOW_ISO,\n",
    "            \"roots\": [str(x) for x in self.roots],\n",
    "            \"outdir\": str(self.outdir),\n",
    "            \"threads\": self.threads,\n",
    "            \"action\": self.action,\n",
    "            \"ocr\": self.do_ocr,\n",
    "            \"unpack\": self.do_unpack,\n",
    "            \"bates\": self.do_bates,\n",
    "            \"bates_prefix\": self.bates_prefix,\n",
    "        }\n",
    "        self.manifest_path.write_text(json.dumps(manifest, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "        # scan and process\n",
    "        records = []\n",
    "        futures = []\n",
    "        with ThreadPoolExecutor(max_workers=self.threads) as ex:\n",
    "            # Get an iterable of files to process, potentially limited\n",
    "            file_iterator = list(\n",
    "                self.iter_files()\n",
    "            )  # Convert to list to get total count for tqdm\n",
    "            for p, root in tqdm(file_iterator, desc=\"Processing\"):\n",
    "                futures.append(ex.submit(self.process_one, p, root))\n",
    "\n",
    "            for fu in tqdm(\n",
    "                as_completed(futures), total=len(futures), desc=\"Processing\"\n",
    "            ):\n",
    "                rec = fu.result()\n",
    "                if rec:\n",
    "                    records.append(rec)\n",
    "\n",
    "        # export csv/jsonl\n",
    "        rows = []\n",
    "        with self.jsonl_path.open(\"w\", encoding=\"utf-8\") as jf:\n",
    "            for r in records:\n",
    "                jf.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "                mj = json.loads(r[\"meta_json\"]) if r.get(\"meta_json\") else {}\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"path\": r[\"path\"],\n",
    "                        \"organized_path\": r.get(\"organized_path\", \"\"),\n",
    "                        \"type\": r.get(\"type\", \"\"),\n",
    "                        \"ext\": r.get(\"ext\", \"\"),\n",
    "                        \"size\": r.get(\"size\", 0),\n",
    "                        \"sha256\": r.get(\"sha256\", \"\"),\n",
    "                        \"mtime\": r.get(\"mtime\", 0),\n",
    "                        \"bates_id\": r.get(\"bates_id\", \"\"),\n",
    "                        \"text_preview\": (\n",
    "                            mj.get(\"text_preview\")\n",
    "                            or mj.get(\"raw_preview\")\n",
    "                            or mj.get(\"ocr_text_preview\")\n",
    "                            or \"\"\n",
    "                        )[:1000],\n",
    "                        \"pages\": mj.get(\"pages\", \"\"),\n",
    "                    }\n",
    "                )\n",
    "        if rows:\n",
    "            pd.DataFrame(rows).to_csv(self.csv_path, index=False)\n",
    "\n",
    "        # summary + duplicates\n",
    "        dup_map = {}\n",
    "        for r in records:\n",
    "            sha = r.get(\"sha256\")\n",
    "            if not sha:\n",
    "                continue\n",
    "            dup_map.setdefault(sha, []).append(r)\n",
    "        duplicates = {k: v for k, v in dup_map.items() if len(v) > 1}\n",
    "\n",
    "        # write duplicate list, and link copies into DEDUP\n",
    "        dups_out = self.outdir / \"duplicates.json\"\n",
    "        dups_out.write_text(json.dumps(duplicates, indent=2), encoding=\"utf-8\")\n",
    "        for sha, items in duplicates.items():\n",
    "            base_dir = ensure_dir(self.dup_dir / sha[:8])\n",
    "            for idx, item in enumerate(items, 1):\n",
    "                src = Path(item.get(\"organized_path\") or item.get(\"path\"))\n",
    "                if not src or not src.exists():\n",
    "                    continue\n",
    "                dst = base_dir / f\"{idx:02d}_{src.name}\"\n",
    "                if not dst.exists():\n",
    "                    try:\n",
    "                        os.link(src, dst)\n",
    "                    except Exception:\n",
    "                        shutil.copy2(src, dst)\n",
    "                self.log_event(\n",
    "                    {\n",
    "                        \"op\": \"dup_collect\",\n",
    "                        \"sha256\": sha,\n",
    "                        \"src\": str(src),\n",
    "                        \"dst\": str(dst),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        # finalize manifest\n",
    "        manifest[\"finished\"] = (\n",
    "            datetime.utcnow().replace(microsecond=0).isoformat() + \"Z\"\n",
    "        )\n",
    "        manifest[\"files_total\"] = len(records)\n",
    "        manifest[\"duplicates\"] = len(duplicates)\n",
    "        self.manifest_path.write_text(json.dumps(manifest, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "        # also export minimal DB snapshot to ensure write success\n",
    "        self.db.commit()\n",
    "        self.db.close()\n",
    "        print(f\"[done] DB: {self.db_path}\")\n",
    "        print(f\"[done] CSV: {self.csv_path}\")\n",
    "        print(f\"[done] JSONL: {self.jsonl_path}\")\n",
    "        print(f\"[done] MANIFEST: {self.manifest_path}\")\n",
    "        print(f\"[done] LEDGER: {self.ledger_path}\")\n",
    "        print(f\"[done] ORGANIZED: {self.org_dir}\")\n",
    "        print(f\"[done] BATES: {self.bates_dir}  (if enabled)\")\n",
    "        print(f\"[done] OCR: {self.ocr_dir}      (if any)\")\n",
    "        print(f\"[done] UNPACKED: {self.unpack_dir} (if enabled)\")\n",
    "        print(f\"[done] DEDUP: {self.dup_dir}\")\n",
    "\n",
    "\n",
    "# ---------- CLI ----------\n",
    "def parse_args():\n",
    "    ap = argparse.ArgumentParser(\n",
    "        description=\"Forensic evidence organizer and processor\"\n",
    "    )\n",
    "    ap.add_argument(\"--root\", nargs=\"+\", required=True, help=\"Root folder(s) to scan\")\n",
    "    ap.add_argument(\"--out\", default=\"OUTPUT\", help=\"Output directory\")\n",
    "    ap.add_argument(\"--threads\", type=int, default=6, help=\"Concurrency level\")\n",
    "    ap.add_argument(\n",
    "        \"--action\",\n",
    "        choices=[\"copy\", \"move\", \"link\"],\n",
    "        default=\"copy\",\n",
    "        help=\"Place files into ORGANIZED with this mode\",\n",
    "    )\n",
    "    ap.add_argument(\"--no-ocr\", action=\"store_true\", help=\"Disable OCR\")\n",
    "    ap.add_argument(\"--unpack\", action=\"store_true\", help=\"Extract archives safely\")\n",
    "    ap.add_argument(\"--no-bates\", action=\"store_true\", help=\"Disable Bates stamping\")\n",
    "    ap.add_argument(\n",
    "        \"--bates\", default=\"BATES\", help=\"Bates prefix, e.g., AJP, EXH, CASE2025\"\n",
    "    )\n",
    "    ap.add_argument(\"--bates-start\", type=int, default=1, help=\"Starting Bates number\")\n",
    "\n",
    "    # For Colab execution, we need to parse args differently or provide defaults.\n",
    "    # We will provide defaults here for demonstration.\n",
    "    # args, unknown = ap.parse_known_args() # parse known args first\n",
    "    # if not args.root:\n",
    "    #     # Provide a default root if none is given (for Colab execution)\n",
    "    #     args.root = [\"/content/drive/MyDrive/litgation_OS$\"] # Example default root directory\n",
    "    # return args\n",
    "    # In Colab, we bypass argparse for direct execution with defaults\n",
    "    class Args:\n",
    "        def __init__(self):\n",
    "            self.root = [\"/content/drive/MyDrive/litgation_OS$\"]\n",
    "            self.out = \"/content/drive/MyDrive/OUTPUT\"  # Modified output path\n",
    "            self.threads = 6\n",
    "            self.action = \"copy\"\n",
    "            self.no_ocr = False\n",
    "            self.unpack = False\n",
    "            self.no_bates = False\n",
    "            self.bates = \"BATES\"\n",
    "            self.bates_start = 1\n",
    "\n",
    "    return Args()\n",
    "\n",
    "\n",
    "# Directly call main function to avoid argparse issues in Colab\n",
    "main()"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-140007588.py:79: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  NOW_ISO = datetime.utcnow().replace(microsecond=0).isoformat()+\"Z\"\n",
      "Processing: 0it [00:00, ?it/s]\n",
      "Processing: 0it [00:00, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[done] DB: /content/drive/MyDrive/OUTPUT/evidence.db\n",
      "[done] CSV: /content/drive/MyDrive/OUTPUT/evidence.csv\n",
      "[done] JSONL: /content/drive/MyDrive/OUTPUT/evidence.jsonl\n",
      "[done] MANIFEST: /content/drive/MyDrive/OUTPUT/MANIFEST.json\n",
      "[done] LEDGER: /content/drive/MyDrive/OUTPUT/LEDGER.jsonl\n",
      "[done] ORGANIZED: /content/drive/MyDrive/OUTPUT/ORGANIZED\n",
      "[done] BATES: /content/drive/MyDrive/OUTPUT/BATES  (if enabled)\n",
      "[done] OCR: /content/drive/MyDrive/OUTPUT/OCR      (if any)\n",
      "[done] UNPACKED: /content/drive/MyDrive/OUTPUT/UNPACKED (if enabled)\n",
      "[done] DEDUP: /content/drive/MyDrive/OUTPUT/ORGANIZED/DEDUP\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "/tmp/ipython-input-140007588.py:652: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  manifest[\"finished\"] = datetime.utcnow().replace(microsecond=0).isoformat()+\"Z\"\n"
     ]
    }
   ]
  }
 ]
}