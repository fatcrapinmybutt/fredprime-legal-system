# ========= LITIGATION PIPELINE: DRIVE → OCR → PARSE → INDEX → QUERY =========
# Usage:
#   1) One-time prerequisites:
#        - Install rclone and configure a remote named "gdrive":   rclone config
#        - Install system OCR tools:
#            Windows (Admin PowerShell):  choco install rclone tesseract ocrmypdf -y
#            Debian/Ubuntu:               sudo apt-get update && sudo apt-get install -y rclone tesseract-ocr ocrmypdf
#            macOS (Homebrew):            brew install rclone tesseract ocrmypdf
#        - Optional: Google Document AI for premium OCR at scale:
#            export GOOGLE_APPLICATION_CREDENTIALS="/path/keys.json"
#            export DOC_AI_PROJECT="your-project-id"
#            export DOC_AI_LOCATION="us"            # or eu, etc.
#            export DOC_AI_PROCESSOR_ID="xxxxxxxxxx" # OCR/Document OCR processor
#
#   2) Create an isolated environment and run the pipeline:
#        python - <<'PY'
#        <PASTE THIS WHOLE PY BLOCK>
#        PY
#
#   3) Commands (inside the Python tool below):
#        python pipeline.py sync                      # rclone mirror from Drive
#        python pipeline.py run                       # OCR→parse→index everything
#        python pipeline.py run --name-contains shady # limit by filename filter
#        python pipeline.py query "lease ledger water overcharge"
#
# Notes:
#  - Processes 10k+ files with resume. Uses a manifest DB to skip unchanged files.
#  - Exports native Google Docs/Sheets/Slides via rclone for downstream parsing.
#  - Stores outputs under: _mirror/  _processed/  _ujsonl/  _index/
# ============================================================================
python - <<'PY'
import os, sys, hashlib, json, shutil, subprocess, time, sqlite3, mimetypes
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed, ProcessPoolExecutor

# ------------------------------ Config ---------------------------------
RCLONE_REMOTE = os.environ.get("RCLONE_REMOTE","gdrive")
MIRROR_DIR    = Path("_mirror").resolve()
PROC_DIR      = Path("_processed").resolve()
JSONL_DIR     = Path("_ujsonl").resolve()
INDEX_DIR     = Path("_index").resolve()
TMP_DIR       = Path("_tmp").resolve()
LOG_DIR       = Path("_logs").resolve()
DB_PATH       = Path("_state/manifest.sqlite").resolve()
STATE_DIR     = DB_PATH.parent

# Tuning
CHECKERS= "8"; TRANSFERS="8"; MAX_WORKERS = max(4, os.cpu_count() or 4)
EXPORT_FORMATS = "docx,xlsx,pptx,odt,ods,odp,pdf"
INCLUDE_EXT = {".pdf",".png",".jpg",".jpeg",".tif",".tiff",".bmp",".webp",".txt",".md",".docx",".xlsx",".pptx",".csv",".rtf",".html",".eml",".json"}

# Optional Google Document AI
DOC_AI_OK = all(os.environ.get(k) for k in ["GOOGLE_APPLICATION_CREDENTIALS","DOC_AI_PROJECT","DOC_AI_LOCATION","DOC_AI_PROCESSOR_ID"])

# --------------------------- Dependency check --------------------------
def ensure_pip(pkgs):
    import importlib
    missing = []
    for p in pkgs:
        mod = p.split("==")[0].split("[")[0].replace("-", "_")
        try: importlib.import_module(mod)
        except Exception: missing.append(p)
    if missing:
        print(f"[setup] Installing: {' '.join(missing)}", flush=True)
        subprocess.check_call([sys.executable, "-m", "pip", "install", "--upgrade", "pip"])
        subprocess.check_call([sys.executable, "-m", "pip", "install", *missing])

ensure_pip([
    "rich==13.7.1",
    "unstructured[all-docs]==0.15.7",
    "pdfminer.six==20240706",
    "Pillow==10.4.0",
    "pytesseract==0.3.13",
    "python-docx==1.1.2",
    "llama-index==0.10.50",
    "google-cloud-documentai==2.30.0" if DOC_AI_OK else "typing_extensions" # no-op alt
])

from rich.console import Console
from rich.progress import Progress, BarColumn, TimeElapsedColumn, TimeRemainingColumn, MofNCompleteColumn
console = Console()

# ----------------------------- State DB --------------------------------
STATE_DIR.mkdir(parents=True, exist_ok=True)
with sqlite3.connect(DB_PATH) as conn:
    conn.execute("""CREATE TABLE IF NOT EXISTS files(
        relpath TEXT PRIMARY KEY,
        size INTEGER,
        mtime REAL,
        sha256 TEXT,
        status TEXT,
        last_error TEXT,
        updated_at REAL
    )""")
    conn.commit()

def db_upsert(rel, size, mtime, sha256, status=None, err=None):
    now = time.time()
    with sqlite3.connect(DB_PATH) as conn:
        conn.execute("""
        INSERT INTO files(relpath,size,mtime,sha256,status,last_error,updated_at)
        VALUES(?,?,?,?,?,?,?)
        ON CONFLICT(relpath) DO UPDATE SET size=?, mtime=?, sha256=?, status=?, last_error=?, updated_at=?""",
        (rel,size,mtime,sha256,status,err,now,size,mtime,sha256,status,err,now))
        conn.commit()

def db_get(rel):
    with sqlite3.connect(DB_PATH) as conn:
        cur = conn.execute("SELECT size,mtime,sha256,status FROM files WHERE relpath=?",(rel,))
        return cur.fetchone()

# ----------------------------- Helpers ---------------------------------
def sh(cmd:list, check=True):
    return subprocess.run(cmd, check=check)

def sha256_file(p:Path, block=1024*1024):
    h=hashlib.sha256()
    with p.open("rb") as f:
        while True:
            b=f.read(block)
            if not b: break
            h.update(b)
    return h.hexdigest()

def is_tool(name):
    from shutil import which
    return which(name) is not None

def safe_rel(p:Path, root:Path)->str:
    return str(p.resolve().relative_to(root))

def needs_processing(src:Path):
    rel = safe_rel(src, MIRROR_DIR)
    stat = src.stat()
    existing = db_get(rel)
    if not existing: return True
    size0, mtime0, sha0, status0 = existing
    # Quick path: size+mtime changed → reprocess
    if int(stat.st_size)!=int(size0) or abs(stat.st_mtime-mtime0)>1:
        return True
    # If already complete, skip
    if status0=="done": return False
    # Else compute hash to be safe
    sha = sha256_file(src)
    return sha != sha0

# ------------------------------ Sync -----------------------------------
def cmd_sync():
    MIRROR_DIR.mkdir(exist_ok=True)
    LOG_DIR.mkdir(exist_ok=True)
    if not is_tool("rclone"):
        console.print("[red]rclone not found. Install and configure it first.[/red]")
        sys.exit(1)
    console.print("[cyan]Syncing Google Drive → local mirror...[/cyan]")
    log = LOG_DIR/"rclone_sync.log"
    cmd = [
        "rclone","sync",f"{RCLONE_REMOTE}:/", str(MIRROR_DIR),
        "--drive-export-formats", EXPORT_FORMATS,
        "--checkers", CHECKERS, "--transfers", TRANSFERS,
        "--fast-list","--progress","--stats","30s","--retries","5","--low-level-retries","10",
        "--log-file", str(log), "--log-level","INFO"
    ]
    sh(cmd, check=True)
    console.print("[green]Sync complete.[/green]")

# ------------------------------ OCR ------------------------------------
def ocr_pdf_docai(src:Path, out_pdf:Path, out_txt:Path):
    # Google Document AI OCR processor
    from google.cloud import documentai as docai
    project = os.environ["DOC_AI_PROJECT"]
    location = os.environ["DOC_AI_LOCATION"]
    processor_id = os.environ["DOC_AI_PROCESSOR_ID"]
    client = docai.DocumentProcessorServiceClient()
    name = client.processor_path(project, location, processor_id)
    content = src.read_bytes()
    raw_document = docai.RawDocument(content=content, mime_type="application/pdf")
    request = docai.ProcessRequest(name=name, raw_document=raw_document)
    result = client.process_document(request=request)
    doc = result.document
    # Text
    out_txt.parent.mkdir(parents=True, exist_ok=True)
    out_txt.write_text(doc.text or "", encoding="utf-8", errors="ignore")
    # Save PDF passthrough for provenance
    out_pdf.parent.mkdir(parents=True, exist_ok=True)
    shutil.copy2(src, out_pdf)

def ocr_pdf_local(src:Path, out_pdf:Path, out_txt:Path):
    # OCRmyPDF with sidecar text
    out_pdf.parent.mkdir(parents=True, exist_ok=True)
    out_txt.parent.mkdir(parents=True, exist_ok=True)
    cmd = ["ocrmypdf","--skip-text","--sidecar", str(out_txt), str(src), str(out_pdf)]
    try:
        sh(cmd, check=True)
    except subprocess.CalledProcessError:
        # Fallback: save original and attempt text extraction via pdfminer
        shutil.copy2(src, out_pdf)
        try:
            from pdfminer.high_level import extract_text
            txt = extract_text(str(src)) or ""
            out_txt.write_text(txt, encoding="utf-8", errors="ignore")
        except Exception:
            out_txt.write_text("", encoding="utf-8")

def ocr_image_local(src:Path, out_txt:Path):
    out_txt.parent.mkdir(parents=True, exist_ok=True)
    try:
        from PIL import Image
        import pytesseract
        text = pytesseract.image_to_string(Image.open(src))
        out_txt.write_text(text or "", encoding="utf-8", errors="ignore")
    except Exception as e:
        out_txt.write_text("", encoding="utf-8")

def passthrough_text(src:Path, out_txt:Path):
    out_txt.parent.mkdir(parents=True, exist_ok=True)
    try:
        out_txt.write_text(src.read_text(encoding="utf-8", errors="ignore"), encoding="utf-8", errors="ignore")
    except Exception:
        out_txt.write_text("", encoding="utf-8")

def process_one(src:Path):
    rel = safe_rel(src, MIRROR_DIR)
    stat = src.stat()
    sha = sha256_file(src)
    out_base = PROC_DIR / src.relative_to(MIRROR_DIR)
    if src.suffix.lower()==".pdf":
        out_pdf = out_base.with_suffix(".pdf")
        out_txt = out_base.with_suffix(".txt")
        try:
            if DOC_AI_OK:
                ocr_pdf_docai(src, out_pdf, out_txt)
            else:
                ocr_pdf_local(src, out_pdf, out_txt)
            db_upsert(rel, stat.st_size, stat.st_mtime, sha, status="done", err=None)
        except Exception as e:
            db_upsert(rel, stat.st_size, stat.st_mtime, sha, status="error", err=str(e))
    elif src.suffix.lower() in {".png",".jpg",".jpeg",".tif",".tiff",".bmp",".webp"}:
        out_txt = out_base.with_suffix(".txt")
        try:
            ocr_image_local(src, out_txt)
            db_upsert(rel, stat.st_size, stat.st_mtime, sha, status="done", err=None)
        except Exception as e:
            db_upsert(rel, stat.st_size, stat.st_mtime, sha, status="error", err=str(e))
    elif src.suffix.lower() in {".txt",".md",".csv",".json",".rtf",".html",".eml"}:
        out_txt = out_base.with_suffix(".txt")
        try:
            passthrough_text(src, out_txt)
            db_upsert(rel, stat.st_size, stat.st_mtime, sha, status="done", err=None)
        except Exception as e:
            db_upsert(rel, stat.st_size, stat.st_mtime, sha, status="error", err=str(e))
    else:
        # For Office exports and unknowns, we rely on Unstructured later using the binary file directly if no .txt
        out_placeholder = out_base  # marker dir
        out_placeholder.parent.mkdir(parents=True, exist_ok=True)
        db_upsert(rel, stat.st_size, stat.st_mtime, sha, status="done", err=None)

# -------------------------- Unstructured → JSONL -----------------------
def unstructured_partition_to_jsonl(src:Path, txt_sidecar:Path):
    from unstructured.partition.auto import partition
    target = (JSONL_DIR / src.relative_to(PROC_DIR)).with_suffix(".jsonl")
    target.parent.mkdir(parents=True, exist_ok=True)
    text_input = None
    if txt_sidecar.exists():
        try:
            text_input = txt_sidecar.read_text(encoding="utf-8", errors="ignore")
        except Exception:
            text_input = None
    try:
        if text_input is not None:
            elements = partition(text=text_input)
        else:
            elements = partition(filename=str(src if src.exists() else txt_sidecar))
        with target.open("w", encoding="utf-8") as f:
            for el in elements:
                f.write(json.dumps(el.to_dict(), ensure_ascii=False) + "\n")
    except Exception as e:
        # write minimal JSONL to keep pipeline moving
        with target.open("w", encoding="utf-8") as f:
            blob = {"type":"Text","text": text_input or f"[unparsed] {src.name}"}
            f.write(json.dumps(blob, ensure_ascii=False) + "\n")

def build_index():
    from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, StorageContext, load_index_from_storage
    # Concatenate JSONL lines into .txt docs for fast ingest
    staging = Path("_rag_docs"); 
    if staging.exists():
        shutil.rmtree(staging)
    staging.mkdir(parents=True, exist_ok=True)
    count=0
    for j in JSONL_DIR.rglob("*.jsonl"):
        txt = staging / j.with_suffix(".txt").name
        txt.parent.mkdir(parents=True, exist_ok=True)
        try:
            lines = j.read_text(encoding="utf-8", errors="ignore").splitlines()
            txt.write_text("\n".join(lines), encoding="utf-8", errors="ignore")
            count+=1
        except Exception:
            pass
    if count==0:
        console.print("[yellow]No JSONL documents to index.[/yellow]")
        return
    reader = SimpleDirectoryReader(str(staging))
    docs = reader.load_data()
    idx = VectorStoreIndex.from_documents(docs)
    if INDEX_DIR.exists():
        shutil.rmtree(INDEX_DIR)
    idx.storage_context.persist(persist_dir=str(INDEX_DIR))
    console.print(f"[green]Index built in {INDEX_DIR}[/green]")

# ------------------------------ Commands --------------------------------
def cmd_run(name_contains:str|None):
    if not MIRROR_DIR.exists():
        console.print("[yellow]_mirror not found. Run: python pipeline.py sync[/yellow]")
        sys.exit(1)
    PROC_DIR.mkdir(parents=True, exist_ok=True)
    JSONL_DIR.mkdir(parents=True, exist_ok=True)
    TMP_DIR.mkdir(parents=True, exist_ok=True)
    LOG_DIR.mkdir(parents=True, exist_ok=True)

    # Enumerate candidate files
    candidates=[]
    for p in MIRROR_DIR.rglob("*"):
        if p.is_file() and p.suffix.lower() in INCLUDE_EXT:
            if name_contains and name_contains.lower() not in p.name.lower(): 
                continue
            if needs_processing(p):
                candidates.append(p)

    console.print(f"[cyan]Files to (re)process: {len(candidates)}[/cyan]")
    # Stage 1: OCR / passthrough
    with Progress("{task.description}", BarColumn(), MofNCompleteColumn(), TimeElapsedColumn(), TimeRemainingColumn(), console=console) as prog:
        t = prog.add_task("OCR/parsing input", total=len(candidates))
        def _wrap(p):
            try:
                process_one(p)
            finally:
                prog.update(t, advance=1)
        with ProcessPoolExecutor(max_workers=min(MAX_WORKERS, 8)) as ex:
            for _ in ex.map(_wrap, candidates):
                pass

    # Stage 2: Unstructured partition to JSONL
    proc_files = []
    for p in PROC_DIR.rglob("*"):
        if p.is_file():
            proc_files.append(p)
    console.print(f"[cyan]Partitioning to JSONL: scanning {len(proc_files)} processed files[/cyan]")
    with Progress("{task.description}", BarColumn(), MofNCompleteColumn(), TimeElapsedColumn(), TimeRemainingColumn(), console=console) as prog:
        t = prog.add_task("Unstructured partition", total=len(proc_files))
        def _u(p:Path):
            try:
                if p.suffix.lower()==".pdf":
                    unstructured_partition_to_jsonl(p, p.with_suffix(".txt"))
                elif p.suffix.lower()==".txt":
                    unstructured_partition_to_jsonl(p, p)
                else:
                    # For binaries without .txt, Unstructured will try filename path
                    unstructured_partition_to_jsonl(p, p.with_suffix(".txt"))
            finally:
                prog.update(t, advance=1)
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:
            for _ in ex.map(_u, proc_files):
                pass

    # Stage 3: Build index
    build_index()
    console.print("[bold green]Pipeline complete.[/bold green]")

def cmd_query(q:str):
    from llama_index.core import StorageContext, load_index_from_storage
    if not INDEX_DIR.exists():
        console.print("[red]Index missing. Run: python pipeline.py run[/red]")
        sys.exit(1)
    storage = StorageContext.from_defaults(persist_dir=str(INDEX_DIR))
    idx = load_index_from_storage(storage)
    ans = idx.as_query_engine().query(q)
    print("\n=== ANSWER ===")
    try:
        print(str(ans))
    except Exception:
        print(ans)

def cmd_errors():
    with sqlite3.connect(DB_PATH) as conn:
        rows = conn.execute("SELECT relpath,last_error FROM files WHERE last_error IS NOT NULL AND last_error!=''").fetchall()
        for rel, err in rows:
            print(rel, "->", err)

# ------------------------------ Entrypoint ------------------------------
# Allow this file to be saved for reuse
Path("pipeline.py").write_text(open(__file__, "r", encoding="utf-8").read(), encoding="utf-8")

if __name__=="__main__":
    import argparse
    ap = argparse.ArgumentParser(description="Drive→OCR→Parse→Index pipeline")
    sp = ap.add_subparsers(dest="cmd")

    sp_sync = sp.add_parser("sync", help="Mirror Google Drive via rclone")
    sp_run  = sp.add_parser("run", help="Process, parse, and index")
    sp_run.add_argument("--name-contains", default=None, help="Only files with this substring in filename")
    sp_q    = sp.add_parser("query", help="Query the local index")
    sp_q.add_argument("text")
    sp_err  = sp.add_parser("errors", help="List processing errors")

    args = ap.parse_args()
    if args.cmd=="sync":
        cmd_sync()
    elif args.cmd=="run":
        cmd_run(args.name_contains)
    elif args.cmd=="query":
        cmd_query(args.text)
    elif args.cmd=="errors":
        cmd_errors()
    else:
        ap.print_help()
PY
